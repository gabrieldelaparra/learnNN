{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de imágenes con YOLO9000\n",
    "**Universidad de Chile**<br>\n",
    "**Facultad de Ciencias, Física y Matemáticas**<br>\n",
    "**Departamento de Ciencias de Computación**<br>\n",
    "**CC5509 - Reconocimiento de Patrones**<br>\n",
    "<br>\n",
    "*Profesores:*<br>\n",
    "**José M. Saavedra R.**<br>\n",
    "**Mauricio Cerda Villablanca**<br>\n",
    "<br>\n",
    "*Ayudante:*<br>\n",
    "**Camila Álvarez I.**<br>\n",
    "<br>\n",
    "*Alumnos:*<br>\n",
    "**Cristobal Dotte**<br>\n",
    "**Gabriel De La Parra**<br>\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este proyecto, se trabajó con [YOLO9000](https://pjreddie.com/darknet/yolo/) para hacer detección y clasificación de objetos en imágenes.\n",
    "\n",
    "El [paper](https://arxiv.org/abs/1612.08242) fue escrito por Joseph Redmon y Ali Farhadi en Diciembre del 2016, por lo que se puede considerar como un trabajo en el *state-of-the-art*.\n",
    "\n",
    "YOLO (You Only Look Once) en la versión YOLO9000 propone varias mejoras al sistema YOLO original de los mismos autores. Esta versión lleva el nombre 9000 ya que puede detectar sobre 9000 clases de objetos. Las mejores a esta versión provienen de sugerencias de otros autores y técnicas implementadas por otros sistemas. \n",
    "\n",
    "Uno de los aspectos más importantes de YOLO, es que puede correr a 67FPS con un mAP (*Mean Average Precision*) del 76.8 en VOC 2007. Este resultado es mucho mejor que otras técnicas existentes actualmente y a una velocidad mucho mayor. Los detalles sobre la comparación se pueden encontrar en el Paper.\n",
    "\n",
    "El objetivo de este proyecto será realizar detección sobre carteles de precios en imágenes de frutas y verduras de ferias libres y la vega.\n",
    "\n",
    "## Procedimiento\n",
    "\n",
    "Se realizó el siguiente procedimiento para lograr el reconocimiento y clasificación de las imágenes:\n",
    "\n",
    "1. Construcción del dataset\n",
    "    1. Obtención de imágenes\n",
    "    2. Preprocesamiento\n",
    "    2. Etiquetado\n",
    "    3. Adaptación de etiquetas\n",
    "2. Entrenamiento\n",
    "    1. Descarga YOLO\n",
    "    2. Compilación\n",
    "    3. Pruebas\n",
    "    4. Archivos de configuración\n",
    "    4. Train y Test Set\n",
    "    5. Entrenamiento\n",
    "3. Pruebas\n",
    "    1. Pruebas\n",
    "    2. Resultados\n",
    "4. Conclusiones\n",
    "\n",
    "## Dataset\n",
    "\n",
    "En este proyecto, se trabajará con un dataset generado desde cero para este proyecto. El dataset está compuesto por 878 imágenes. Las fotos fueron etiquetadas ocupando la herramienta [LabelImg](https://github.com/tzutalin/labelImg). Dicha herramienta tiene un formato de salida que no es el mismo que requiere YOLO. Fue necesario crear un script para convertir los archivos de salida.\n",
    "\n",
    "### Obtención de imágenes\n",
    "\n",
    "Las imágenes fueron tomadas en la vega y en dos ferias libres de Santiago de Chile.\n",
    "\n",
    "A continuación se presenta un sample de algunas de estas fotos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/116.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/125.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/248.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/287.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/326.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/363.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/415.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/446.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/51.jpg' /><img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='feria labeled/78.jpg' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "from glob import glob\n",
    "imagesList=''.join( [\"<img style='width: 300px; margin: 2px; float: left; border: 1px solid black;' src='%s' />\" % str(s) \n",
    "                 for s in sorted(glob('feria labeled/*.jpg')[1:100:10]) ])\n",
    "display(HTML(imagesList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Cada imagen tiene originalmente nombres como: `Foto 03-11-17 14 47 04.jpg` y tiene una resolución de `3264x2448`\n",
    "\n",
    "### Resize\n",
    "Si bien la resolución inicial tiene más detalle, no es necesario tanta resolución para entrenar las imagenes, debido a que YOLO hace transformación de las imagenes. Otro problema de trabajar con las imagenes a tan gran resolución, es que copiarlas al servidor ocupa mucho más recursos/tiempo. El set original tiene 2.5gb. El nuevo, 90 mb. \n",
    "\n",
    "Como se verá más adelante, se entrenó con la resolución de `418x418`. Se redujo en tamaño al `20%`, que corresponde a `693x490`.\n",
    "\n",
    "``` bash\n",
    "$ for a in *.jpg; do convert \"$a\" -resize 20% resized/\"$a\"; done\n",
    "```\n",
    "\n",
    "![resize](images/resize.png)\n",
    "\n",
    "### Rename\n",
    "Los nombres de las imagenes no influyen mucho, sin embargo son maś complicados para seleccionar al momento de usarlos para testear. Por lo mismo, se renombraron las imagenes de forma secuencial. El código que se utilizó es el siguiente.\n",
    "\n",
    "``` bash\n",
    "$ ls | cat -n | while read n f; do mv \"$f\" \"$n.jpg\"; done\n",
    "```\n",
    "\n",
    "![rename](images/rename.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etiquetado\n",
    "\n",
    "Se utilizó la herramienta [LabelImg](https://github.com/tzutalin/labelImg) para etiquetar las imagenes. A continuación se presenta una muestra de la interfáz gráfica de la herramienta:\n",
    "\n",
    "![labelimg](images/labelimg.png)\n",
    "\n",
    "Uno de los beneficios de utilizar esta herramienta, es que se pueden utilizar distintas clases para las imágenes. Adicionalmente, la herramienta tiene comandos:\n",
    "- `w`: crear un cuadro\n",
    "- `a`: imagen anterior\n",
    "- `d`: imagen siguiente\n",
    "\n",
    "Por cada imagen, se crea un archivo de formato xml. El archivo tiene el siguiente contenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\r\n",
      "\t<folder>feria</folder>\r\n",
      "\t<filename>1.jpg</filename>\r\n",
      "\t<path>/home/clgadel/labeler/feria/1.jpg</path>\r\n",
      "\t<source>\r\n",
      "\t\t<database>Unknown</database>\r\n",
      "\t</source>\r\n",
      "\t<size>\r\n",
      "\t\t<width>653</width>\r\n",
      "\t\t<height>490</height>\r\n",
      "\t\t<depth>3</depth>\r\n",
      "\t</size>\r\n",
      "\t<segmented>0</segmented>\r\n",
      "\t<object>\r\n",
      "\t\t<name>manzana</name>\r\n",
      "\t\t<pose>Unspecified</pose>\r\n",
      "\t\t<truncated>0</truncated>\r\n",
      "\t\t<difficult>0</difficult>\r\n",
      "\t\t<bndbox>\r\n",
      "\t\t\t<xmin>192</xmin>\r\n",
      "\t\t\t<ymin>285</ymin>\r\n",
      "\t\t\t<xmax>227</xmax>\r\n",
      "\t\t\t<ymax>317</ymax>\r\n",
      "\t\t</bndbox>\r\n",
      "\t</object>\r\n"
     ]
    }
   ],
   "source": [
    "!head -25 1.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptación de etiquetas\n",
    "\n",
    "Para el procesamiento de las etiquetas utilizaremos la librería [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"1.xml\", \"r\"), \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<annotation>\n",
      "<folder>feria</folder>\n",
      "<filename>1.jpg</filename>\n",
      "<path>/home/clgadel/labeler/feria/1.jpg</path>\n",
      "<source>\n",
      "<database>Unknown</database>\n",
      "</source>\n",
      "<size>\n",
      "<width>653</width>\n",
      "<height>490</height>\n",
      "<depth>3</depth>\n",
      "</size>\n",
      "<segmented>0</segmented>\n",
      "<object>\n",
      "<name>manzana</name>\n",
      "<pose>Unspecified</pose>\n",
      "<truncated>0</truncated>\n",
      "<difficult>0</difficult>\n",
      "<bndbox>\n",
      "<xmin>192</xmin>\n",
      "<ymin>285</ymin>\n",
      "<xmax>227</xmax>\n",
      "<ymax>317</ymax>\n",
      "</bndbox>\n",
      "</object>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(soup)[0:496])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ubicar Etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = soup.find_all(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<object>\n",
       "<name>manzana</name>\n",
       "<pose>Unspecified</pose>\n",
       "<truncated>0</truncated>\n",
       "<difficult>0</difficult>\n",
       "<bndbox>\n",
       "<xmin>202</xmin>\n",
       "<ymin>305</ymin>\n",
       "<xmax>240</xmax>\n",
       "<ymax>340</ymax>\n",
       "</bndbox>\n",
       "</object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombre de la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manzana\n"
     ]
    }
   ],
   "source": [
    "obj = objects[0]\n",
    "print(obj.find(\"name\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordenadas de la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manzana 192 285\n"
     ]
    }
   ],
   "source": [
    "obj = objects[0]\n",
    "print(obj.find(\"name\").text, obj.bndbox.xmin.text, obj.bndbox.ymin.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamaño de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<size>\n",
       "<width>653</width>\n",
       "<height>490</height>\n",
       "<depth>3</depth>\n",
       "</size>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = soup.annotation.size\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653 490\n"
     ]
    }
   ],
   "source": [
    "print(size.width.text, size.height.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión al formato necesario\n",
    "\n",
    "YOLO necesita de un formato necesario para trabajar. Este formato, según la página debe ser del siguiente tipo:\n",
    "\n",
    "```\n",
    "Generate Labels for VOC\n",
    "\n",
    "Now we need to generate the label files that Darknet uses. Darknet wants a .txt file for each image with a line for each ground truth object in the image that looks like:\n",
    "\n",
    "<object-class> <x> <y> <width> <height>\n",
    "Where x, y, width, and height are relative to the image's width and height.\n",
    "```\n",
    "\n",
    "Existe [un repositorio](https://github.com/Guanghan/darknet/blob/master/scripts/convert.py) para hacer la conversión de las imágenes, pero funciona con otro tipo y fue necesario adaptarlo. A continuación los cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\t\t\t 653 490\n",
      "Size porcentual:\t 0.0015313935681470138 0.0020408163265306124\n",
      "\n",
      "Bounding box:\t\t 192 285 227 317\n",
      "Dimensiones YOLO:\t 0.3208269525267994 0.6142857142857143 0.053598774885145486 0.0653061224489796\n"
     ]
    }
   ],
   "source": [
    "size = soup.annotation.size\n",
    "dw, dh = int(size.width.text), int(size.height.text)\n",
    "print(\"Size:\\t\\t\\t\", dw, dh)\n",
    "dw, dh = 1./int(size.width.text), 1./int(size.height.text)\n",
    "print(\"Size porcentual:\\t\", dw, dh)\n",
    "print()\n",
    "obj = objects[0]\n",
    "print(\"Bounding box:\\t\\t\", obj.bndbox.xmin.text, obj.bndbox.ymin.text, obj.bndbox.xmax.text, obj.bndbox.ymax.text)\n",
    "x, y = (int(obj.bndbox.xmin.text) + int(obj.bndbox.xmax.text))/2, (int(obj.bndbox.ymin.text) + int(obj.bndbox.ymax.text))/2\n",
    "w, h = int(obj.bndbox.xmax.text)-int(obj.bndbox.xmin.text), int(obj.bndbox.ymax.text)-int(obj.bndbox.ymin.text)\n",
    "\n",
    "x = x*dw\n",
    "y = y*dh\n",
    "w = w*dw\n",
    "h = h*dh\n",
    "\n",
    "print(\"Dimensiones YOLO:\\t\", x, y, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manzana 0.3208269525267994 0.6142857142857143 0.053598774885145486 0.0653061224489796\n",
      "manzana 0.33843797856049007 0.6581632653061225 0.05819295558958652 0.07142857142857144\n",
      "manzana 0.34915773353751917 0.6948979591836736 0.06125574272588055 0.0673469387755102\n",
      "manzana 0.26339969372128635 0.6948979591836736 0.06738131699846861 0.0673469387755102\n",
      "manzana 0.2840735068912711 0.7459183673469388 0.06278713629402757 0.07959183673469389\n",
      "manzana 0.33843797856049007 0.7622448979591837 0.06431852986217458 0.07142857142857144\n",
      "manzana 0.3973966309341501 0.7326530612244898 0.05972434915773354 0.07346938775510205\n",
      "manzana 0.20444104134762633 0.7438775510204082 0.06584992343032159 0.06326530612244899\n",
      "manzana 0.4004594180704441 0.653061224489796 0.05053598774885146 0.07346938775510205\n",
      "manzana 0.35298621745788666 0.5979591836734695 0.03215926493108729 0.0489795918367347\n",
      "manzana 0.37059724349157736 0.5877551020408164 0.030627871362940276 0.04489795918367347\n",
      "manzana 0.4058192955589587 0.5836734693877551 0.03981623277182236 0.05714285714285715\n",
      "manzana 0.41500765696784075 0.6142857142857143 0.03981623277182236 0.04489795918367347\n",
      "manzana 0.442572741194487 0.6744897959183674 0.042879019908116385 0.05918367346938776\n",
      "manzana 0.45941807044410415 0.6306122448979592 0.033690658499234305 0.0489795918367347\n",
      "manzana 0.47166921898928027 0.5908163265306123 0.02756508422664625 0.04693877551020409\n",
      "manzana 0.495405819295559 0.6081632653061225 0.02909647779479326 0.036734693877551024\n",
      "precio 0.2113323124042879 0.5020408163265306 0.11332312404287903 0.09387755102040818\n",
      "precio 0.47090352220520676 0.5040816326530613 0.10872894333843798 0.1346938775510204\n",
      "precio 0.1416539050535988 0.8867346938775511 0.0781010719754977 0.1653061224489796\n",
      "precio 0.711332312404288 0.3163265306122449 0.1638591117917305 0.1673469387755102\n",
      "precio 0.9609494640122512 0.4744897959183674 0.0781010719754977 0.12040816326530614\n",
      "precio 0.9203675344563553 0.916326530612245 0.15926493108728945 0.15510204081632656\n",
      "precio 0.22511485451761104 0.33571428571428574 0.10413476263399694 0.1163265306122449\n",
      "precio 0.4517611026033691 0.33979591836734696 0.10107197549770292 0.12040816326530614\n",
      "precio 0.34762633996937214 0.4061224489795919 0.08575803981623277 0.10204081632653061\n",
      "precio 0.09571209800918837 0.31938775510204087 0.03828483920367535 0.07551020408163266\n",
      "precio 0.20367534456355285 0.2653061224489796 0.07044410413476264 0.0653061224489796\n",
      "precio 0.36983154670750384 0.24387755102040817 0.06891271056661562 0.09183673469387756\n",
      "platano 0.05130168453292496 0.05918367346938776 0.09035222052067382 0.1142857142857143\n",
      "platano 0.1753445635528331 0.07142857142857144 0.12710566615620214 0.13877551020408163\n",
      "platano 0.4793261868300153 0.047959183673469394 0.07963246554364473 0.09183673469387756\n"
     ]
    }
   ],
   "source": [
    "size = soup.annotation.size\n",
    "dw, dh = int(size.width.text), int(size.height.text)\n",
    "dw, dh = 1./int(size.width.text), 1./int(size.height.text)\n",
    "\n",
    "for obj in objects:\n",
    "    x, y = (int(obj.bndbox.xmin.text) + int(obj.bndbox.xmax.text))/2, (int(obj.bndbox.ymin.text) + int(obj.bndbox.ymax.text))/2\n",
    "    w, h = int(obj.bndbox.xmax.text)-int(obj.bndbox.xmin.text), int(obj.bndbox.ymax.text)-int(obj.bndbox.ymin.text)\n",
    "\n",
    "    x = x*dw\n",
    "    y = y*dh\n",
    "    w = w*dw\n",
    "    h = h*dh\n",
    "    print(obj.find(\"name\").text, x, y, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manzana 0.3208269525267994 0.6142857142857143 0.053598774885145486 0.0653061224489796\r\n",
      "manzana 0.33843797856049007 0.6581632653061225 0.05819295558958652 0.07142857142857144\r\n",
      "manzana 0.34915773353751917 0.6948979591836736 0.06125574272588055 0.0673469387755102\r\n",
      "manzana 0.26339969372128635 0.6948979591836736 0.06738131699846861 0.0673469387755102\r\n",
      "manzana 0.2840735068912711 0.7459183673469388 0.06278713629402757 0.07959183673469389\r\n",
      "manzana 0.33843797856049007 0.7622448979591837 0.06431852986217458 0.07142857142857144\r\n",
      "manzana 0.3973966309341501 0.7326530612244898 0.05972434915773354 0.07346938775510205\r\n",
      "manzana 0.20444104134762633 0.7438775510204082 0.06584992343032159 0.06326530612244899\r\n",
      "manzana 0.4004594180704441 0.653061224489796 0.05053598774885146 0.07346938775510205\r\n",
      "manzana 0.35298621745788666 0.5979591836734695 0.03215926493108729 0.0489795918367347\r\n",
      "manzana 0.37059724349157736 0.5877551020408164 0.030627871362940276 0.04489795918367347\r\n",
      "manzana 0.4058192955589587 0.5836734693877551 0.03981623277182236 0.05714285714285715\r\n",
      "manzana 0.41500765696784075 0.6142857142857143 0.03981623277182236 0.04489795918367347\r\n",
      "manzana 0.442572741194487 0.6744897959183674 0.042879019908116385 0.05918367346938776\r\n",
      "manzana 0.45941807044410415 0.6306122448979592 0.033690658499234305 0.0489795918367347\r\n",
      "manzana 0.47166921898928027 0.5908163265306123 0.02756508422664625 0.04693877551020409\r\n",
      "manzana 0.495405819295559 0.6081632653061225 0.02909647779479326 0.036734693877551024\r\n",
      "precio 0.2113323124042879 0.5020408163265306 0.11332312404287903 0.09387755102040818\r\n",
      "precio 0.47090352220520676 0.5040816326530613 0.10872894333843798 0.1346938775510204\r\n",
      "precio 0.1416539050535988 0.8867346938775511 0.0781010719754977 0.1653061224489796\r\n",
      "precio 0.711332312404288 0.3163265306122449 0.1638591117917305 0.1673469387755102\r\n",
      "precio 0.9609494640122512 0.4744897959183674 0.0781010719754977 0.12040816326530614\r\n",
      "precio 0.9203675344563553 0.916326530612245 0.15926493108728945 0.15510204081632656\r\n",
      "precio 0.22511485451761104 0.33571428571428574 0.10413476263399694 0.1163265306122449\r\n",
      "precio 0.4517611026033691 0.33979591836734696 0.10107197549770292 0.12040816326530614\r\n",
      "precio 0.34762633996937214 0.4061224489795919 0.08575803981623277 0.10204081632653061\r\n",
      "precio 0.09571209800918837 0.31938775510204087 0.03828483920367535 0.07551020408163266\r\n",
      "precio 0.20367534456355285 0.2653061224489796 0.07044410413476264 0.0653061224489796\r\n",
      "precio 0.36983154670750384 0.24387755102040817 0.06891271056661562 0.09183673469387756\r\n",
      "platano 0.05130168453292496 0.05918367346938776 0.09035222052067382 0.1142857142857143\r\n",
      "platano 0.1753445635528331 0.07142857142857144 0.12710566615620214 0.13877551020408163\r\n",
      "platano 0.4793261868300153 0.047959183673469394 0.07963246554364473 0.09183673469387756\r\n"
     ]
    }
   ],
   "source": [
    "with open(\"1.txt\", \"w\") as f: \n",
    "    for obj in objects:\n",
    "        x, y = (int(obj.bndbox.xmin.text) + int(obj.bndbox.xmax.text))/2, (int(obj.bndbox.ymin.text) + int(obj.bndbox.ymax.text))/2\n",
    "        w, h = int(obj.bndbox.xmax.text)-int(obj.bndbox.xmin.text), int(obj.bndbox.ymax.text)-int(obj.bndbox.ymin.text)\n",
    "\n",
    "        x = x*dw\n",
    "        y = y*dh\n",
    "        w = w*dw\n",
    "        h = h*dh\n",
    "        f.write(\" \".join((obj.find(\"name\").text, str(x), str(y), str(w), str(h))))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "!cat 1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['163.xml',\n",
       " '156.xml',\n",
       " '166.xml',\n",
       " '236.xml',\n",
       " '47.xml',\n",
       " '272.xml',\n",
       " '134.xml',\n",
       " '372.xml',\n",
       " '25.xml',\n",
       " '385.xml']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_files = [f for f in os.listdir(\"labels\") if f.endswith('.xml')]\n",
    "xml_files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos a TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_files = [f for f in os.listdir(\"labels\") if f.endswith('.xml')]\n",
    "for xml_file in xml_files:\n",
    "    soup = BeautifulSoup(open(\"labels/\"+xml_file, \"r\"), \"xml\")\n",
    "    size = soup.annotation.size\n",
    "    dw, dh = int(size.width.text), int(size.height.text)\n",
    "    dw, dh = 1./int(size.width.text), 1./int(size.height.text)\n",
    "    \n",
    "    objects = soup.find_all(\"object\")\n",
    "    output_file = xml_file.replace(\".xml\", \".txt\")\n",
    "    with open(\"txt_labels/\"+output_file, \"w\") as f: \n",
    "        for obj in objects:\n",
    "            x, y = (int(obj.bndbox.xmin.text) + int(obj.bndbox.xmax.text))/2, (int(obj.bndbox.ymin.text) + int(obj.bndbox.ymax.text))/2\n",
    "            w, h = int(obj.bndbox.xmax.text)-int(obj.bndbox.xmin.text), int(obj.bndbox.ymax.text)-int(obj.bndbox.ymin.text)\n",
    "\n",
    "            x = x*dw\n",
    "            y = y*dh\n",
    "            w = w*dw\n",
    "            h = h*dh\n",
    "            f.write(\" \".join((\"0\", str(x), str(y), str(w), str(h))))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que **las imagenes y los archivos .txt deben quedar en la misma carpeta.**\n",
    "\n",
    "## Entrenamiento\n",
    "\n",
    "Para el entrenamiento se compone de 5 etapas:\n",
    "\n",
    "1. Descarga YOLO\n",
    "2. Compilación\n",
    "3. Pruebas\n",
    "4. Archivos de configuración\n",
    "5. Train y Test Set\n",
    "6. Entrenamiento\n",
    "\n",
    "Durante nuestro proceso, nos guiamos de [este tutorial](https://timebutt.github.io/static/how-to-train-yolov2-to-detect-custom-objects/) (En Windows) y [este también](http://guanghan.info/blog/en/my-works/train-yolo/) (en YOLO1). Tuvimos que realizar varias modificaciones.\n",
    "\n",
    "### Descargar YOLO\n",
    "\n",
    "Para obtener YOLO, se puede clonar el repositorio usando git:\n",
    "\n",
    "```\n",
    "git clone https://github.com/pjreddie/darknet\n",
    "```\n",
    "\n",
    "### Compilación\n",
    "\n",
    "La compilación es relativamente sencilla.\n",
    "\n",
    "```\n",
    "cd darknet\n",
    "make\n",
    "```\n",
    "\n",
    "Lo anterior compilará YOLO son con capacidades de CPU. Primero trabajaremos con CPU, probaremos el sistema y luego compilaremos con GPU.\n",
    "\n",
    "### Pruebas\n",
    "\n",
    "Para las pruebas, una vez compilado, es necesario descargar unos pesos ya entrenados. Desde la página del autor:\n",
    "\n",
    "```\n",
    "wget https://pjreddie.com/media/files/yolo.weights\n",
    "```\n",
    "\n",
    "Una vez descargados, se puede correr el siguiente comando:\n",
    "\n",
    "```\n",
    "./darknet detector test cfg/coco.data cfg/yolo.cfg yolo.weights data/dog.jpg\n",
    "```\n",
    "\n",
    "Esto dará un resultado del siguiente tipo:\n",
    "```\n",
    "layer     filters    size              input                output\n",
    "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
    "    1 max          2 x 2 / 2   416 x 416 x  32   ->   208 x 208 x  32\n",
    "    .......\n",
    "   29 conv    425  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 425\n",
    "   30 detection\n",
    "Loading weights from yolo.weights...Done!\n",
    "data/dog.jpg: Predicted in 0.016287 seconds.\n",
    "car: 54%\n",
    "bicycle: 51%\n",
    "dog: 56%\n",
    "```\n",
    "\n",
    "![](images/dog.png)\n",
    "\n",
    "Si esto funciona, haremos la compilación para GPU. \n",
    "\n",
    "Para esto, es necesario editar el archivo `Makefile` y colocar `GPU=1` y `CUDNN=1`.\n",
    "\n",
    "![](images/makefile.png)\n",
    "\n",
    "Una vez modificado, se puede correr el siguiente código:\n",
    "\n",
    "```\n",
    "make clean\n",
    "make\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos de configuración\n",
    "\n",
    "Para entrenar el sistema con nuestras propias imagenes, debemos modificar 3 archivos:\n",
    "- d.data\n",
    "- n.names\n",
    "- c.cfg\n",
    "\n",
    "Nuestro archivos quedan como se ven a continuación:\n",
    "\n",
    "**Data**\n",
    "\n",
    "En el archivo de `data` colocaremos:\n",
    "- la cantidad de clases\n",
    "- el set de entrenamiento y set de pruebas\n",
    "- los nombres de las etiquetas (recordar que las etiquetas tienen números)\n",
    "- la ruta donde se guardarán los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes = 1\r\n",
      "train = train.txt\r\n",
      "valid = test.txt\r\n",
      "names = n.names\r\n",
      "backup = backup/\r\n"
     ]
    }
   ],
   "source": [
    "!cat d.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names**\n",
    "\n",
    "En el archivo `names` colocaremos los nombres de las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARTEL FERIA\r\n"
     ]
    }
   ],
   "source": [
    "!cat n.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arquitectura**\n",
    "\n",
    "La arquitectura, la copiaremos desde el archivo `yolo-voc.2.0.cfg` y modificaremos lo siguiente:\n",
    "\n",
    "- **linea 230:** `classes=1`\n",
    "- **linea 224:** `filters=(classes + 5)*5` en nuestro caso `filters=30`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[net]\r\n",
      "batch=64\r\n",
      "subdivisions=8\r\n",
      "height=416\r\n",
      "width=416\r\n",
      "channels=3\r\n",
      "momentum=0.9\r\n",
      "decay=0.0005\r\n",
      "angle=0\r\n",
      "saturation = 1.5\r\n",
      "exposure = 1.5\r\n",
      "hue=.1\r\n",
      "\r\n",
      "learning_rate=0.0001\r\n",
      "max_batches = 45000\r\n",
      "policy=steps\r\n",
      "steps=100,25000,35000\r\n",
      "scales=10,.1,.1\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=32\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[maxpool]\r\n",
      "size=2\r\n",
      "stride=2\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=64\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[maxpool]\r\n",
      "size=2\r\n",
      "stride=2\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=128\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=64\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=128\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[maxpool]\r\n",
      "size=2\r\n",
      "stride=2\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=256\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=128\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=256\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[maxpool]\r\n",
      "size=2\r\n",
      "stride=2\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=512\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=256\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=512\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=256\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=512\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[maxpool]\r\n",
      "size=2\r\n",
      "stride=2\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=1024\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=512\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=1024\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=512\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "filters=1024\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "\r\n",
      "#######\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "filters=1024\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "filters=1024\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[route]\r\n",
      "layers=-9\r\n",
      "\r\n",
      "[reorg]\r\n",
      "stride=2\r\n",
      "\r\n",
      "[route]\r\n",
      "layers=-1,-3\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "batch_normalize=1\r\n",
      "size=3\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "filters=1024\r\n",
      "activation=leaky\r\n",
      "\r\n",
      "[convolutional]\r\n",
      "size=1\r\n",
      "stride=1\r\n",
      "pad=1\r\n",
      "filters=30\r\n",
      "activation=linear\r\n",
      "\r\n",
      "[region]\r\n",
      "anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52\r\n",
      "bias_match=1\r\n",
      "classes=1\r\n",
      "coords=4\r\n",
      "num=5\r\n",
      "softmax=1\r\n",
      "jitter=.2\r\n",
      "rescore=1\r\n",
      "\r\n",
      "object_scale=5\r\n",
      "noobject_scale=1\r\n",
      "class_scale=1\r\n",
      "coord_scale=1\r\n",
      "\r\n",
      "absolute=1\r\n",
      "thresh = .6\r\n",
      "random=0\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat c.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train y Test Set\n",
    "\n",
    "Para entrenar YOLO, es necesario tener 2 listados de archivos. Un set de entrenamiento y un set de pruebas. Ambos archivos se pueden crear con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clgadel/Projects/pat/Project\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Directory where the data will reside, relative to 'darknet.exe'\n",
    "path_data = '/feria labeled/'\n",
    "\n",
    "# Percentage of images to be used for the test set\n",
    "percentage_test = 10;\n",
    "\n",
    "# Create and/or truncate train.txt and test.txt\n",
    "file_train = open('train.txt', 'w')  \n",
    "file_test = open('test.txt', 'w')\n",
    "\n",
    "# Populate train.txt and test.txt\n",
    "counter = 1  \n",
    "index_test = round(100 / percentage_test)  \n",
    "for pathAndFilename in glob.iglob(os.path.join(current_dir + path_data, \"*.jpg\")):  \n",
    "    title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n",
    "\n",
    "    if counter == index_test:\n",
    "        counter = 1\n",
    "        file_test.write(path_data + title + '.jpg' + \"\\n\")\n",
    "    else:\n",
    "        file_train.write(path_data + title + '.jpg' + \"\\n\")\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/feria labeled/77.jpg\r\n",
      "/feria labeled/446.jpg\r\n",
      "/feria labeled/132.jpg\r\n",
      "/feria labeled/362.jpg\r\n",
      "/feria labeled/221.jpg\r\n",
      "/feria labeled/18.jpg\r\n",
      "/feria labeled/3.jpg\r\n",
      "/feria labeled/9.jpg\r\n",
      "/feria labeled/35.jpg\r\n",
      "/feria labeled/38.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/feria labeled/473.jpg\r\n",
      "/feria labeled/61.jpg\r\n",
      "/feria labeled/180.jpg\r\n",
      "/feria labeled/267.jpg\r\n",
      "/feria labeled/84.jpg\r\n",
      "/feria labeled/126.jpg\r\n",
      "/feria labeled/337.jpg\r\n",
      "/feria labeled/94.jpg\r\n",
      "/feria labeled/353.jpg\r\n",
      "/feria labeled/27.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Una vez generados las etiquetas, los archivos de configuración y los sets, es necesario descargar un set de pesos iniciales para poder entrenar. Estos los descagaremos desde el siguiente link: [darknet19_448.conv.23](https://pjreddie.com/media/files/darknet19_448.conv.23)\n",
    "\n",
    "Una vez descargados, se puede proceder a entrenar el sistema. \n",
    "\n",
    "Para realizar esto, podemos ejecutar el siguiente comando:\n",
    "\n",
    "`./darknet detector train d.data c.cfg darknet19_448.conv.23`\n",
    "\n",
    "El output del entrenamiento es del siguiente tipo:\n",
    "\n",
    "```\n",
    "Learning Rate: 0.0001, Momentum: 0.9, Decay: 0.0005\n",
    "Loaded: 2.315760 seconds\n",
    "Region Avg IOU: 0.217734, Class: 1.000000, Obj: 0.419592, No Obj: 0.468766, Avg Recall: 0.043478,  count: 23\n",
    "Region Avg IOU: 0.222831, Class: 1.000000, Obj: 0.454939, No Obj: 0.469173, Avg Recall: 0.045455,  count: 22\n",
    "Region Avg IOU: 0.265696, Class: 1.000000, Obj: 0.499332, No Obj: 0.469689, Avg Recall: 0.100000,  count: 20\n",
    "Region Avg IOU: 0.190703, Class: 1.000000, Obj: 0.518117, No Obj: 0.468631, Avg Recall: 0.000000,  count: 23\n",
    "Region Avg IOU: 0.265896, Class: 1.000000, Obj: 0.457795, No Obj: 0.468984, Avg Recall: 0.130435,  count: 23\n",
    "Region Avg IOU: 0.229292, Class: 1.000000, Obj: 0.508706, No Obj: 0.468891, Avg Recall: 0.074074,  count: 27\n",
    "Region Avg IOU: 0.230069, Class: 1.000000, Obj: 0.419362, No Obj: 0.469978, Avg Recall: 0.068966,  count: 29\n",
    "Region Avg IOU: 0.242352, Class: 1.000000, Obj: 0.481317, No Obj: 0.469159, Avg Recall: 0.000000,  count: 20\n",
    "1: 16.841396, 16.841396 avg, 0.000100 rate, 16.321371 seconds, 64 images\n",
    "Loaded: 0.000065 seconds\n",
    "Region Avg IOU: 0.218620, Class: 1.000000, Obj: 0.456032, No Obj: 0.414498, Avg Recall: 0.041667,  count: 24\n",
    "Region Avg IOU: 0.258897, Class: 1.000000, Obj: 0.430074, No Obj: 0.414035, Avg Recall: 0.045455,  count: 22\n",
    "Region Avg IOU: 0.237696, Class: 1.000000, Obj: 0.459021, No Obj: 0.414099, Avg Recall: 0.080000,  count: 25\n",
    "Region Avg IOU: 0.178128, Class: 1.000000, Obj: 0.444677, No Obj: 0.415388, Avg Recall: 0.000000,  count: 33\n",
    "Region Avg IOU: 0.301570, Class: 1.000000, Obj: 0.427377, No Obj: 0.414479, Avg Recall: 0.121212,  count: 33\n",
    "Region Avg IOU: 0.276301, Class: 1.000000, Obj: 0.469463, No Obj: 0.414910, Avg Recall: 0.076923,  count: 26\n",
    "Region Avg IOU: 0.242387, Class: 1.000000, Obj: 0.368439, No Obj: 0.415005, Avg Recall: 0.041667,  count: 24\n",
    "Region Avg IOU: 0.208118, Class: 1.000000, Obj: 0.464884, No Obj: 0.414942, Avg Recall: 0.000000,  count: 22\n",
    "2: 14.087240, 16.565981 avg, 0.000100 rate, 17.360945 seconds, 128 images\n",
    "...\n",
    "```\n",
    "Cada línea Region, indica las `subdivision`(8) del `batch`(64).\n",
    "- Region Avg IOU: Intersection of Union.\n",
    "- Obj: Probabilidad de que haya un objeto.\n",
    "- No Obj: Probabilidad de que no haya un objeto.\n",
    "\n",
    "Luego de cada `batch` se entrega un estado del entrenamiento:\n",
    "`1: 16.841396, 16.841396 avg, 0.000100 rate, 16.321371 seconds, 64 images`\n",
    "\n",
    "Lo ideal del entrenamiento, es que el segundo número, error promedio, (16.841396 en la línea anterior) baje lo más posible.\n",
    "\n",
    "El proceso de entrenamiento para 1 clase consume alrededor de 3gb de memoria de la GPU. Esto puede ser revisado con el comando `nvidia-smi`\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 384.66                 Driver Version: 384.66                    |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla K40c          Off  | 00000000:02:00.0 Off |                  Off |\n",
    "| 29%   62C    P0   167W / 235W |   2965MiB / 12205MiB |     99%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Tesla K40c          Off  | 00000000:03:00.0 Off |                  Off |\n",
    "| 23%   29C    P8    20W / 235W |     11MiB / 12205MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  Tesla K40c          Off  | 00000000:81:00.0 Off |                  Off |\n",
    "| 23%   33C    P8    24W / 235W |     11MiB / 12205MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  Tesla K40c          Off  | 00000000:82:00.0 Off |                  Off |\n",
    "| 26%   51C    P0    65W / 235W |     12MiB / 12205MiB |     18%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID  Type  Process name                               Usage      |\n",
    "|=============================================================================|\n",
    "|    0     10392    C   ./darknet                                     2952MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "En nuestro caso, entrenamos el sistema en segundo plano con el comando `screen`. \n",
    "\n",
    "```\n",
    "screen\n",
    "./darknet detector train d.data c.cfg darknet19_448.conv.23 > log.txt\n",
    "[ctrl+a, d] : salir de screen\n",
    "```\n",
    "\n",
    "Luego de 2 horas, se han realizado ~ 1400 iteraciones, el error ha bajado a `0.166`:\n",
    "\n",
    "```\n",
    "Loaded: 0.000042 seconds\n",
    "Region Avg IOU: 0.665527, Class: 1.000000, Obj: 0.598194, No Obj: 0.006928, Avg Recall: 0.757576,  count: 33\n",
    "Region Avg IOU: 0.789315, Class: 1.000000, Obj: 0.726305, No Obj: 0.005664, Avg Recall: 1.000000,  count: 27\n",
    "Region Avg IOU: 0.808586, Class: 1.000000, Obj: 0.686888, No Obj: 0.006079, Avg Recall: 0.947368,  count: 19\n",
    "Region Avg IOU: 0.839705, Class: 1.000000, Obj: 0.727469, No Obj: 0.006832, Avg Recall: 1.000000,  count: 18\n",
    "Region Avg IOU: 0.824412, Class: 1.000000, Obj: 0.702527, No Obj: 0.007220, Avg Recall: 0.956522,  count: 23\n",
    "Region Avg IOU: 0.759820, Class: 1.000000, Obj: 0.642817, No Obj: 0.006916, Avg Recall: 0.861111,  count: 36\n",
    "Region Avg IOU: 0.815900, Class: 1.000000, Obj: 0.758524, No Obj: 0.005814, Avg Recall: 0.944444,  count: 18\n",
    "Region Avg IOU: 0.807832, Class: 1.000000, Obj: 0.777660, No Obj: 0.005036, Avg Recall: 0.947368,  count: 19\n",
    "1389: 0.213024, 0.166752 avg, 0.001000 rate, 8.264515 seconds, 88896 images\n",
    "Loaded: 0.000030 seconds\n",
    "Region Avg IOU: 0.595099, Class: 1.000000, Obj: 0.505556, No Obj: 0.005770, Avg Recall: 0.714286,  count: 35\n",
    "Region Avg IOU: 0.840390, Class: 1.000000, Obj: 0.762315, No Obj: 0.006210, Avg Recall: 1.000000,  count: 20\n",
    "Region Avg IOU: 0.740536, Class: 1.000000, Obj: 0.678102, No Obj: 0.005565, Avg Recall: 0.900000,  count: 30\n",
    "Region Avg IOU: 0.852529, Class: 1.000000, Obj: 0.826861, No Obj: 0.005790, Avg Recall: 1.000000,  count: 12\n",
    "Region Avg IOU: 0.799913, Class: 1.000000, Obj: 0.693582, No Obj: 0.007936, Avg Recall: 1.000000,  count: 39\n",
    "Region Avg IOU: 0.783571, Class: 1.000000, Obj: 0.699744, No Obj: 0.004345, Avg Recall: 1.000000,  count: 18\n",
    "Region Avg IOU: 0.835393, Class: 1.000000, Obj: 0.767473, No Obj: 0.005715, Avg Recall: 1.000000,  count: 21\n",
    "Region Avg IOU: 0.807125, Class: 1.000000, Obj: 0.735216, No Obj: 0.006527, Avg Recall: 0.954545,  count: 22\n",
    "1390: 0.160165, 0.166093 avg, 0.001000 rate, 8.175913 seconds, 88960 images\n",
    "```\n",
    "\n",
    "El output del entrenamiento lo tiramos a un archivo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo-v2\r\n",
      "\r\n",
      " seen 32 \r\n",
      "Learning Rate: 0.0001, Momentum: 0.9, Decay: 0.0005\r\n",
      "Loaded: 2.315760 seconds\r\n",
      "Region Avg IOU: 0.217734, Class: 1.000000, Obj: 0.419592, No Obj: 0.468766, Avg Recall: 0.043478,  count: 23\r\n",
      "Region Avg IOU: 0.222831, Class: 1.000000, Obj: 0.454939, No Obj: 0.469173, Avg Recall: 0.045455,  count: 22\r\n",
      "Region Avg IOU: 0.265696, Class: 1.000000, Obj: 0.499332, No Obj: 0.469689, Avg Recall: 0.100000,  count: 20\r\n",
      "Region Avg IOU: 0.190703, Class: 1.000000, Obj: 0.518117, No Obj: 0.468631, Avg Recall: 0.000000,  count: 23\r\n",
      "Region Avg IOU: 0.265896, Class: 1.000000, Obj: 0.457795, No Obj: 0.468984, Avg Recall: 0.130435,  count: 23\r\n",
      "Region Avg IOU: 0.229292, Class: 1.000000, Obj: 0.508706, No Obj: 0.468891, Avg Recall: 0.074074,  count: 27\r\n",
      "Region Avg IOU: 0.230069, Class: 1.000000, Obj: 0.419362, No Obj: 0.469978, Avg Recall: 0.068966,  count: 29\r\n",
      "Region Avg IOU: 0.242352, Class: 1.000000, Obj: 0.481317, No Obj: 0.469159, Avg Recall: 0.000000,  count: 20\r\n",
      "1: 16.841396, 16.841396 avg, 0.000100 rate, 16.321371 seconds, 64 images\r\n",
      "Loaded: 0.000065 seconds\r\n",
      "Region Avg IOU: 0.218620, Class: 1.000000, Obj: 0.456032, No Obj: 0.414498, Avg Recall: 0.041667,  count: 24\r\n",
      "Region Avg IOU: 0.258897, Class: 1.000000, Obj: 0.430074, No Obj: 0.414035, Avg Recall: 0.045455,  count: 22\r\n",
      "Region Avg IOU: 0.237696, Class: 1.000000, Obj: 0.459021, No Obj: 0.414099, Avg Recall: 0.080000,  count: 25\r\n",
      "Region Avg IOU: 0.178128, Class: 1.000000, Obj: 0.444677, No Obj: 0.415388, Avg Recall: 0.000000,  count: 33\r\n",
      "Region Avg IOU: 0.301570, Class: 1.000000, Obj: 0.427377, No Obj: 0.414479, Avg Recall: 0.121212,  count: 33\r\n",
      "Region Avg IOU: 0.276301, Class: 1.000000, Obj: 0.469463, No Obj: 0.414910, Avg Recall: 0.076923,  count: 26\r\n",
      "Region Avg IOU: 0.242387, Class: 1.000000, Obj: 0.368439, No Obj: 0.415005, Avg Recall: 0.041667,  count: 24\r\n",
      "Region Avg IOU: 0.208118, Class: 1.000000, Obj: 0.464884, No Obj: 0.414942, Avg Recall: 0.000000,  count: 22\r\n",
      "2: 14.087240, 16.565981 avg, 0.000100 rate, 17.360945 seconds, 128 images\r\n",
      "Loaded: 0.000055 seconds\r\n"
     ]
    }
   ],
   "source": [
    "!head -25 log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region Avg IOU: 0.824412, Class: 1.000000, Obj: 0.702527, No Obj: 0.007220, Avg Recall: 0.956522,  count: 23\r\n",
      "Region Avg IOU: 0.759820, Class: 1.000000, Obj: 0.642817, No Obj: 0.006916, Avg Recall: 0.861111,  count: 36\r\n",
      "Region Avg IOU: 0.815900, Class: 1.000000, Obj: 0.758524, No Obj: 0.005814, Avg Recall: 0.944444,  count: 18\r\n",
      "Region Avg IOU: 0.807832, Class: 1.000000, Obj: 0.777660, No Obj: 0.005036, Avg Recall: 0.947368,  count: 19\r\n",
      "1389: 0.213024, 0.166752 avg, 0.001000 rate, 8.264515 seconds, 88896 images\r\n",
      "Loaded: 0.000030 seconds\r\n",
      "Region Avg IOU: 0.595099, Class: 1.000000, Obj: 0.505556, No Obj: 0.005770, Avg Recall: 0.714286,  count: 35\r\n",
      "Region Avg IOU: 0.840390, Class: 1.000000, Obj: 0.762315, No Obj: 0.006210, Avg Recall: 1.000000,  count: 20\r\n",
      "Region Avg IOU: 0.740536, Class: 1.000000, Obj: 0.678102, No Obj: 0.005565, Avg Recall: 0.900000,  count: 30\r\n",
      "Region Avg IOU: 0.852529, Class: 1.000000, Obj: 0.826861, No Obj: 0.005790, Avg Recall: 1.000000,  count: 12\r\n",
      "Region Avg IOU: 0.799913, Class: 1.000000, Obj: 0.693582, No Obj: 0.007936, Avg Recall: 1.000000,  count: 39\r\n",
      "Region Avg IOU: 0.783571, Class: 1.000000, Obj: 0.699744, No Obj: 0.004345, Avg Recall: 1.000000,  count: 18\r\n",
      "Region Avg IOU: 0.835393, Class: 1.000000, Obj: 0.767473, No Obj: 0.005715, Avg Recall: 1.000000,  count: 21\r\n",
      "Region Avg IOU: 0.807125, Class: 1.000000, Obj: 0.735216, No Obj: 0.006527, Avg Recall: 0.954545,  count: 22\r\n",
      "1390: 0.160165, 0.166093 avg, 0.001000 rate, 8.175913 seconds, 88960 images\r\n",
      "Loaded: 0.000041 seconds\r\n",
      "Region Avg IOU: 0.790488, Class: 1.000000, Obj: 0.755256, No Obj: 0.006820, Avg Recall: 1.000000,  count: 23\r\n",
      "Region Avg IOU: 0.851338, Class: 1.000000, Obj: 0.776919, No Obj: 0.005858, Avg Recall: 1.000000,  count: 18\r\n",
      "Region Avg IOU: 0.815728, Class: 1.000000, Obj: 0.786660, No Obj: 0.006159, Avg Recall: 1.000000,  count: 20\r\n",
      "Region Avg IOU: 0.819428, Class: 1.000000, Obj: 0.710120, No Obj: "
     ]
    }
   ],
   "source": [
    "!tail -20 log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "Ahora mostraremos gráficamente el avance de nuestro entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
